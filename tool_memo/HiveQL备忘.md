Hive SQL常用操作
===

Index
---
<!-- TOC -->

- [正则](#正则)
    - [正则抽取](#正则抽取)
    - [转义问题](#转义问题)
- [常用参数](#常用参数)
- [字符串截取](#字符串截取)
- [窗口函数-排序](#窗口函数-排序)
- [分段采样](#分段采样)
- [获取星期](#获取星期)
- [行转列、侧视图](#行转列侧视图)
- [case when](#case-when)
- [加载自定义 UDF](#加载自定义-udf)
- [加载 transform](#加载-transform)
    - [临时 pip 模块](#临时-pip-模块)
    - [添加额外参数来选择调用函数](#添加额外参数来选择调用函数)
- [临时表](#临时表)
- [建表](#建表)
- [加载 hdfs 数据](#加载-hdfs-数据)
- [查看表信息](#查看表信息)

<!-- /TOC -->

### 正则
> Java 正则表达式 | 菜鸟教程 | https://www.runoob.com/java/java-regular-expressions.html
```
中文：[\\u4e00-\\u9fa5]
2字以上中文： '^[\\u4e00-\\u9fa5]{2,}$'
非（ASC可见字符&中文）：[^\\x21-\\x7e\\u4e00-\\u9fa5]
```

#### 正则抽取
```sql
regexp_extract(mention, '(.*?)-(.*?)', 1)  -- 抽取第一组
regexp_extract(mention, '(.*?)-(.*?)', 2)  -- 抽取第二组
```

#### 转义问题
- 转义要双斜杠 `\\`（XT平台 SPARK 模式下）
    - 一个斜杠的转义是给 SQL 看的，比如 `\;`，两个斜杠才是给匹配方法看的 `\\"`
```sql
-- 示例内容：'{\"cityId\":1}' -> '{"cityId":1}'
regexp_replace(txt, '\\\\"', '"')  -- 这里要用4个`\`，`\\`匹配`\`，`\\"`匹配`"`（sql里面`"`也需要转义）
```


### 常用参数
```sql
SET mapred.reduce.tasks=64;
SET mapred.max.split.size=1024000;          -- 决定每个map处理的最大的文档大小，单位为B，越小 map 越多
SET mapred.min.split.size.per.node=512000;  -- 节点中可以处理的最小的文档大小（可以考虑设置为 max 的一半）
SET mapred.min.split.size.per.rack=512000;  -- 机架中可以处理的最小的文档大小（与 node 相同）
```


### 字符串截取
```sql
-- 截取子串
substr(s, start, length) -- 从 start 截取 length 长度的子串，start 从 1 开始
substr(s, start)  -- 从 start 到末尾，start 可以为负数

-- 示例
substr('abcde', 1, 3)  -- abc
substr('abcde', 2, 3)  -- bc
substr('abcde', 2)  -- bcde
substr('abcde', -2)  -- de
substr('abcde', -3, 2)  -- cd
```

### 窗口函数-排序
> HIVE SQL奇技淫巧 - 知乎 | https://zhuanlan.zhihu.com/p/80887746
```sql
SELECT 
    cookieid,
    createtime,
    pv,
    RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1,         -- 1,1,3,3,5
    DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2,   -- 1,1,2,2,3
    ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn3    -- 1,2,3,4,5
    NTILE(4) OVER(ORDER BY pv) AS rn4  --将所有数据分成4片
FROM lxw1234 
WHERE cookieid = 'cookie1';

cookieid day           pv       rn1     rn2     rn3     rn4 
----------------------------------------------------------- 
cookie1 2015-04-12      7       1       1       1       1
cookie1 2015-04-11      5       2       2       2       1
cookie1 2015-04-15      4       3       3       3       2
cookie1 2015-04-16      4       3       3       4       2
cookie1 2015-04-13      3       5       4       5       3
cookie1 2015-04-14      2       6       5       6       3
cookie1 2015-04-10      1       7       6       7       4
```


### 分段采样
```sql
SELECT query, qv, heap
FROM (
    SELECT query, qv, heap, row_number() over(partition by heap order by rand()) row_num  -- 打乱每堆内部
    FROM (
        SELECT query, qv, ntile(10) over(order by qv desc) AS heap  -- 按 qv 降序分成 10 堆
        FROM table_query_qv
    ) A
) B
WHERE row_num <= 300  -- 每堆采样 300 条
;
```


### 获取星期
```sql
pmod(datediff('#data#', '2012-01-01'), 7)  -- 返回 0-6 表示 星期日-星期六
```


### 行转列、侧视图
```sql
-- hive
LATERAL VIEW EXPLODE(split(sstr,'\\|')) X AS sth  -- `|` 需要转义
LATERAL VIEW EXPLODE(mentionlist) X AS mention
LATERAL VIEW EXPLODE(array(1,2,3,4)) X AS i

-- presto
CROSS JOIN UNNEST(arr1) AS tmp_table (a1)  -- unzip
CROSS JOIN UNNEST(split(label_id, '|')) AS tmp_table (iid)  -- presto 环境下 `|` 不需要转义
CROSS JOIN UNNEST(arr1, arr2) AS tmp_table (a1, a2)  -- unzip

```

### case when
```
case 
    when RequestInfo like '%mtMerger%' and source='2' then 'mt'
    when source = '1' then 'mt'
    when source = '3' then 'other' 
    else 'dp' 
end
```

### 加载自定义 UDF
```
add jar viewfs://hadoop-meituan/user/hadoop-aipnlp/user_upload/andrew.lu_nlp_tools-1.0.jar;
-- 字符串归一化（simple）
CREATE TEMPORARY FUNCTION simpleProcess AS 'com.sankuai.aipnlp.hive.udf.QuerySimpleProcess';
-- 分词
CREATE TEMPORARY FUNCTION segment AS 'com.sankuai.aipnlp.hive.udf.MTSegment';

-- ssh://git@git.sankuai.com/~huayang04/entitylink_tools.git
add jar viewfs://hadoop-meituan/user/hadoop-aipnlp/user_upload/huayang04_huay_udf.jar;
add jar viewfs://hadoop-meituan/user/hadoop-aipnlp/user_upload/huayang04_entitylink_udf.jar;

-- 加载 jar 包
add jar viewfs:///user/hadoop-udf-hub/etl-huayang04_entitylink_udf/huayang04_entitylink_udf-online.jar;
-- 字符串归一化（simple）
CREATE TEMPORARY FUNCTION simpleNormalize AS 'com.sankuai.aipnlp.hive.udf.SimpleNormalizeUDF';
-- 字符串归一化
CREATE TEMPORARY FUNCTION textNormalize AS 'com.sankuai.aipnlp.hive.udf.TextNormalizeUDF';
-- 判断子集（Set）
CREATE TEMPORARY FUNCTION isSubset AS 'com.sankuai.aipnlp.hive.udf.IsSubsetUDF';
-- 判断子集（List）
CREATE TEMPORARY FUNCTION isSubList AS 'com.sankuai.aipnlp.hive.udf.IsSubListUDF';
-- 判断子串
CREATE TEMPORARY FUNCTION isSubString AS 'com.sankuai.aipnlp.hive.udf.IsSubStringUDF';
-- 判断子序列
CREATE TEMPORARY FUNCTION isSubSequence AS 'com.sankuai.aipnlp.hive.udf.IsSubSequenceUDF';
-- 获取交集
CREATE TEMPORARY FUNCTION getIntersection AS 'com.sankuai.aipnlp.hive.udf.GetIntersectionUDF';
-- 最长重复子串
CREATE TEMPORARY FUNCTION maxDuplicateSubstring AS 'com.sankuai.aipnlp.hive.udf.MaxDuplicateSubstringUDF';
-- 包含停用词
CREATE TEMPORARY FUNCTION containStopword AS 'com.sankuai.aipnlp.hive.udf.ContainStopwordUDF';
-- 组合
CREATE TEMPORARY FUNCTION getCombination AS 'com.sankuai.aipnlp.hive.udf.GetCombinationUDF';
-- 美团分词
CREATE TEMPORARY FUNCTION mtSegment AS 'com.sankuai.aipnlp.hive.udf.MTSegmentUDF';
-- 字级别 jaccard 相似度
CREATE TEMPORARY FUNCTION wordJaccardSimilarity AS 'com.sankuai.aipnlp.hive.udf.WordJaccardSimilarityUDF';
-- 判断 sequence 中是否包含 branchname 的字符，且不在 shopname 中
CREATE TEMPORARY FUNCTION isContainBranchname AS 'com.sankuai.aipnlp.hive.udf.IsContainBranchnameUDF';
-- 滑动窗口 split（'_'分割）
CREATE TEMPORARY FUNCTION windowSplit AS 'com.sankuai.aipnlp.hive.udf.WindowSplitUDF';
-- java split（支持正则切分，'_'分割）
CREATE TEMPORARY FUNCTION splitUDF AS 'com.sankuai.aipnlp.hive.udf.SplitUDF';
```

### 加载 transform
```
add file $Script('xxx.py');

transform ()
using '/usr/bin/python2.7 xxx.py'
as ()
```

#### 临时 pip 模块
```python
# print(os.getcwd())
tmp_dir = './tmp_dir'
os.mkdir(tmp_dir)
sys.stdout = open(os.devnull, 'w+')  # 输出重定向，防止 pip 的信息输出到 hive 表中
pip.main(['install', 'pypinyin', '-t', tmp_dir, '--ignore-installed'])  # -t 表示安装到指定位置
sys.path.append(tmp_dir)  # 添加环境变量
sys.stdout = sys.__stdout__  # 还原输出重定向

import pypinyin
# run()

pip.main(['uninstall', 'pypinyin', '-y'])  # -y 表示直接确认
shutil.rmtree(tmp_dir)
```

#### 添加额外参数来选择调用函数
```python
# 调用方式：
## using '/usr/bin/python2.7 xxx.py --a'

if len(sys.argv) <= 1:
    raise Exception('--a or --b must be set')
args = tuple(sys.argv[2:])
if sys.argv[1] == '--a':
    run_a()
elif sys.argv[1] == '--b':
    run_b()
```


### 临时表
```sql
-- 上线时使用（Hive 环境下不支持）
CACHE TABLE `tmphuay_子表名` AS
;

-- 测试时使用
DROP TABLE IF EXISTS `${tmpdb}`.tmphuay_子表名;
CREATE TABLE `${tmpdb}`.tmphuay_子表名 AS
;

-- 不写建表语句
DROP TABLE IF EXISTS `${target.table}`;
CREATE TABLE `${target.table}` STORED AS ORC AS
;
```

### 建表
```sql
CREATE TABLE IF NOT EXISTS `$target.table`
(
    `entityID` string COMMENT 'entityID',
    `sequence` string COMMENT 'sequence'
)
COMMENT ''
PARTITIONED BY (hp_stat_date string comment '天分区，yyyy-mm-dd')
ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY '\t'
    LINES TERMINATED BY '\n'
STORED AS textfile
-- STORED AS ORC
-- LOCATION "/user/hadoop-aipnlp/entitylink_entity";
```


### 加载 hdfs 数据
```sql
LOAD DATA INPATH 'viewfs://hadoop-meituan/user/hadoop-aipnlp/huayang04/el_trm/pred__a__b__all.txt'
OVERWRITE into table `${tmpdb}`.tmphuay_entitylink_trm_sample_bert_feature__a__b__tmp;
```

### 查看表信息
```sql
show create table `table_name`
```